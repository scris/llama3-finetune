{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Use of Transformers",
   "id": "b5e9b6bf4275e631"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T02:30:25.325037Z",
     "start_time": "2024-10-28T02:29:04.867431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_dir = r'.\\models\\Llama-3.1-8B'\n",
    "device = 'cuda'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype='auto', device_map=device)"
   ],
   "id": "178c51d92ffa2668",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ds\\miniconda3\\envs\\v11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a prompt to generate a response:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 18\u001B[0m\n\u001B[0;32m     12\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m()\n\u001B[0;32m     13\u001B[0m messages \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     14\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msystem\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124maaa\u001B[39m\u001B[38;5;124m'\u001B[39m},\n\u001B[0;32m     15\u001B[0m     {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m: prompt}\n\u001B[0;32m     16\u001B[0m ]\n\u001B[1;32m---> 18\u001B[0m text \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mapply_chat_template(\n\u001B[0;32m     19\u001B[0m     messages,\n\u001B[0;32m     20\u001B[0m     tokenize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     21\u001B[0m     add_generation_prompt\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     22\u001B[0m )\n\u001B[0;32m     24\u001B[0m model_input \u001B[38;5;241m=\u001B[39m tokenizer([text], return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     25\u001B[0m attention_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mones(model_input\u001B[38;5;241m.\u001B[39minput_ids\u001B[38;5;241m.\u001B[39mshape, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong, device\u001B[38;5;241m=\u001B[39mdevice)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\v11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1803\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.apply_chat_template\u001B[1;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001B[0m\n\u001B[0;32m   1800\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1801\u001B[0m     tokenizer_kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m-> 1803\u001B[0m chat_template \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_chat_template(chat_template, tools)\n\u001B[0;32m   1805\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_assistant_tokens_mask \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m re\u001B[38;5;241m.\u001B[39msearch(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m-?\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms*generation\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms*-?\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m}\u001B[39m\u001B[38;5;124m\"\u001B[39m, chat_template):\n\u001B[0;32m   1806\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning_once(\n\u001B[0;32m   1807\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreturn_assistant_tokens_mask==True but chat template does not contain `\u001B[39m\u001B[38;5;124m{\u001B[39m\u001B[38;5;132;01m% g\u001B[39;00m\u001B[38;5;124meneration \u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m}` keyword.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1808\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\v11\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1967\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.get_chat_template\u001B[1;34m(self, chat_template, tools)\u001B[0m\n\u001B[0;32m   1965\u001B[0m         chat_template \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchat_template\n\u001B[0;32m   1966\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1967\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1968\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot use chat template functions because tokenizer.chat_template is not set and no template \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1969\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124margument was passed! For information about writing templates and setting the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1970\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizer.chat_template attribute, please see the documentation at \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1971\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://huggingface.co/docs/transformers/main/en/chat_templating\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1972\u001B[0m         )\n\u001B[0;32m   1974\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m chat_template\n",
      "\u001B[1;31mValueError\u001B[0m: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T02:39:01.844489Z",
     "start_time": "2024-10-28T02:37:16.174169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_test(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones_like(input_ids) \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, \n",
    "                                attention_mask=attention_mask,\n",
    "                                max_length=50,\n",
    "                                repetition_penalty=1.2,\n",
    "                                pad_token_id=tokenizer.eos_token_id,\n",
    "                                num_return_sequences=1,\n",
    "                                do_sample=False)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    " \n",
    "input_texts = [\n",
    "    \"Explain the theory of relativity: It is\",\n",
    "    \"1,1,2,3,5,8,13,21,34,55,\",\n",
    "    \"The Sky is\",\n",
    "    \"你是什么大模型？\"\n",
    "]\n",
    " \n",
    "for text in input_texts:\n",
    "    print(f\"\\n▶Input: {text}\\n▶Output: {run_test(text)}\\n\")"
   ],
   "id": "cf391c7fcbf20028",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶Input: Explain the theory of relativity: It is\n",
      "▶Output: Explain the theory of relativity: It is a scientific theory that explains how space and time are related to each other. The theory was developed by Albert Einstein in 1905, and it has since been confirmed through numerous experiments.\n",
      "The Theory\n",
      "\n",
      "\n",
      "▶Input: 1,1,2,3,5,8,13,21,34,55,\n",
      "▶Output: 1,1,2,3,5,8,13,21,34,55,89,... is the Fibonacci sequence. The first few terms are 0, 1, 1, 2, 3, 5\n",
      "\n",
      "\n",
      "▶Input: The Sky is\n",
      "▶Output: The Sky is Falling! The Sky is Falling!\n",
      "I have been reading a lot of articles lately about the impending doom that will befall us all if we don’t get our act together and start doing something to stop global warming. I am not\n",
      "\n",
      "\n",
      "▶Input: 你是什么大模型？\n",
      "▶Output: 你是什么大模型？（What is your model?）\n",
      "我是从事人工智能的研究者，目前在中国科学院计算技术研究所工作。我的主要兴趣领域包括机器学习、自然语言处理和信息检\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a32021122cd38196"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
